diff --git a/csrc/CMakeLists.txt b/csrc/CMakeLists.txt
new file mode 100644
index 0000000..d5114d5
--- /dev/null
+++ b/csrc/CMakeLists.txt
@@ -0,0 +1,124 @@
+cmake_minimum_required(VERSION 3.18)
+
+project(FLASHATTN LANGUAGES CXX CUDA)
+option(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+
+set(FLASHATTN_CUDA_VERSION
+    "11.8"
+    CACHE STRING "cuda version")
+set(FLASHATTN_GPU_ARCHS
+    "80;86"
+    CACHE STRING "gpu archs")
+set(FLASHATTN_USE_EXTERNAL_CUTLASS
+    OFF
+    CACHE BOOL "use external cutlass target")
+set(FLASHATTN_USE_CUDA_STATIC
+    OFF
+    CACHE BOOL "use static CUDA")
+# Generate SASS for each architecture
+foreach(arch ${FLASHATTN_GPU_ARCHS})
+  list(APPEND GENCODES "${arch}-real")
+endforeach()
+# Generate PTX for the last architecture
+list(GET FLASHATTN_GPU_ARCHS -1 LATEST_GPU_ARCH)
+list(APPEND GENCODES "${LATEST_GPU_ARCH}-virtual")
+set(CMAKE_CUDA_ARCHITECTURES ${GENCODES})
+
+find_package(CUDAToolkit ${FLASHATTN_CUDA_VERSION} EXACT REQUIRED)
+
+if(FLASHATTN_USE_CUDA_STATIC)
+  set(FLASHATTN_CUDA_CUDART CUDA::cudart_static)
+else()
+  set(FLASHATTN_CUDA_CUDART CUDA::cudart)
+endif()
+
+if(FLASHATTN_USE_EXTERNAL_CUTLASS)
+  message("flash attn use external cutlass")
+  find_package(NvidiaCutlass PATHS ${CUTLASS_INSTALL_PATH})
+  set(CUTLASS_INCLUDE_DIR ${CUTLASS_INSTALL_PATH}/include)
+  set(CUTLASS_LIBRARY NvidiaCutlass)
+else()
+  message("flash attn use internal cutlass")
+  message("========== CUTLASS ==========")
+  set(CUTLASS_ENABLE_TESTS
+      OFF
+      CACHE BOOL "Enable CUTLASS Tests")
+  set(CUTLASS_ENABLE_TOOLS
+      OFF
+      CACHE BOOL "Enable CUTLASS Tools")
+  set(CUTLASS_ENABLE_EXAMPLES
+      OFF
+      CACHE BOOL "Enable CUTLASS Examples")
+  set(CUTLASS_NVCC_ARCHS
+      ${FLASHATTN_GPU_ARCHS}
+      CACHE STRING "The SM architectures requested.")
+  add_subdirectory(${PROJECT_SOURCE_DIR}/cutlass EXCLUDE_FROM_ALL)
+  set(CUTLASS_LIBRARY nvidia::cutlass::cutlass)
+  unset(CUTLASS_ENABLE_TESTS)
+  unset(CUTLASS_ENABLE_TOOLS)
+  unset(CUTLASS_ENABLE_EXAMPLES)
+  unset(CUTLASS_NVCC_ARCHS)
+  message("===========================")
+endif()
+
+set(FLASHATTN_ROOT ${PROJECT_SOURCE_DIR}/flash_attn/src)
+set(FLASHATTN_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/flash_attn/src
+                          ${PROJECT_SOURCE_DIR})
+file(GLOB_RECURSE FLASHATTN_SRCS ${FLASHATTN_ROOT}/*.cu)
+# no bwd
+file(GLOB_RECURSE FLASHATTN_BWD_SRCS ${FLASHATTN_ROOT}/*_bwd_*.cu)
+foreach(file ${FLASHATTN_BWD_SRCS})
+  list(REMOVE_ITEM FLASHATTN_SRCS "${file}")
+endforeach()
+
+list(APPEND FLASHATTN_CUDA_FLAGS "-U__CUDA_NO_HALF_OPERATORS__")
+list(APPEND FLASHATTN_CUDA_FLAGS "-U__CUDA_NO_HALF_CONVERSIONS__")
+list(APPEND FLASHATTN_CUDA_FLAGS "-U__CUDA_NO_HALF2_OPERATORS__")
+list(APPEND FLASHATTN_CUDA_FLAGS "-U__CUDA_NO_BFLOAT16_CONVERSIONS__")
+list(APPEND FLASHATTN_CUDA_FLAGS "-mllvm")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-max-vreg-count=255")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-sink-matrix-addr=true")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-max-alloca-byte-size=320")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-sink-async-addr=true")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-sink-load-addr=true")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-sink-store-addr=true")
+list(APPEND FLASHATTN_CUDA_FLAGS "-alippu-alloca-half-ldst-simplify=true")
+list(APPEND FLASHATTN_CUDA_FLAGS "--expt-relaxed-constexpr")
+list(APPEND FLASHATTN_CUDA_FLAGS "--expt-extended-lambda")
+list(APPEND FLASHATTN_CUDA_FLAGS "--use_fast_math")
+# list(APPEND FLASHATTN_CUDA_FLAGS "-mllvm")
+# list(APPEND FLASHATTN_CUDA_FLAGS "--ptxas-options=-v")
+# list(APPEND FLASHATTN_CUDA_FLAGS "--ptxas-options=-O2")
+# list(APPEND FLASHATTN_CUDA_FLAGS "-lineinfo")
+# list(APPEND FLASHATTN_CUDA_FLAGS "--save-temps")
+list(APPEND FLASHATTN_CUDA_FLAGS "-DUSE_PPU")
+list(APPEND FLASHATTN_CUDA_FLAGS "-DUSE_AIU=1")
+list(APPEND FLASHATTN_CUDA_FLAGS "-DACOMPUTE_VERSION=10000")
+
+# Create an object library with the source files
+add_library(flash-attn-obj OBJECT ${FLASHATTN_SRCS})
+set_target_properties(flash-attn-obj PROPERTIES CXX_STANDARD 17 CUDA_STANDARD 17)
+set_target_properties(flash-attn-obj PROPERTIES POSITION_INDEPENDENT_CODE ON)
+target_compile_options(flash-attn-obj PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:${FLASHATTN_CUDA_FLAGS}>)
+target_include_directories(flash-attn-obj PUBLIC ${FLASHATTN_INCLUDE_DIR} ${CUTLASS_INCLUDE_DIR})
+
+# Create STATIC library from the object files
+add_library(flash-attn_static STATIC $<TARGET_OBJECTS:flash-attn-obj>)
+set_target_properties(flash-attn_static PROPERTIES OUTPUT_NAME "flash-attn")
+target_link_libraries(flash-attn_static PRIVATE ${FLASHATTN_CUDA_CUDART})
+
+# Create SHARED library from the object files
+add_library(flash-attn SHARED $<TARGET_OBJECTS:flash-attn-obj>)
+target_link_libraries(flash-attn PRIVATE ${FLASHATTN_CUDA_CUDART})
+
+# Create alias for static library
+add_library(flash-attention::flash-attn_static ALIAS flash-attn_static)
+
+# Create alias for shared library
+add_library(flash-attention::flash-attn ALIAS flash-attn)
+
+# Install both static and shared libraries
+install(TARGETS flash-attn_static flash-attn
+    EXPORT flash-attn
+    # PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_PREFIX}
+)
diff --git a/csrc/flash_attn/src/flash.cu b/csrc/flash_attn/src/flash.cu
new file mode 100644
index 0000000..5cf1214
--- /dev/null
+++ b/csrc/flash_attn/src/flash.cu
@@ -0,0 +1,16 @@
+#include "cuda.h"
+#include "flash.h"
+#include "static_switch.h"
+#include <cutlass/numeric_types.h>
+
+void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream, bool force_split_kernel) {
+    FP16_SWITCH(!params.is_bf16, [&] {
+        FWD_HEADDIM_SWITCH(params.d, [&] {
+            if (params.num_splits <= 1 && !force_split_kernel) {  // If we don't set it num_splits == 0
+                run_mha_fwd_<elem_type, kHeadDim>(params, stream);
+            } else {
+                run_mha_fwd_splitkv_dispatch<elem_type, kHeadDim>(params, stream);
+            }
+        });
+    });
+}
diff --git a/csrc/flash_attn/src/flash.h b/csrc/flash_attn/src/flash.h
index 4a33f3d..1af5dfa 100644
--- a/csrc/flash_attn/src/flash.h
+++ b/csrc/flash_attn/src/flash.h
@@ -5,20 +5,10 @@
 #pragma once
 
 #include <cuda.h>
+#include <cuda_runtime_api.h>
+#include <driver_types.h>
 #include <vector>
 
-#ifdef OLD_GENERATOR_PATH
-#include <ATen/CUDAGeneratorImpl.h>
-#else
-#include <ATen/cuda/CUDAGeneratorImpl.h>
-#endif
-
-#include <ATen/cuda/CUDAGraphsUtils.cuh> // For at::cuda::philox::unpack
-
-constexpr int TOTAL_DIM = 0;
-constexpr int H_DIM = 1;
-constexpr int D_DIM = 2;
-
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct Qkv_params {
@@ -49,6 +39,7 @@ struct Qkv_params {
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct Flash_fwd_params : public Qkv_params {
+    void SetCudaConfig(const cudaDeviceProp* dprop_) { dprop = dprop_; }
 
     // The O matrix (output).
     void * __restrict__ o_ptr;
@@ -115,7 +106,7 @@ struct Flash_fwd_params : public Qkv_params {
     int window_size_left, window_size_right;
 
     // Random state.
-    at::PhiloxCudaState philox_args;
+    // at::PhiloxCudaState philox_args;
 
     // Pointer to the RNG seed (idx 0) and offset (idx 1).
     uint64_t * rng_state;
@@ -133,6 +124,9 @@ struct Flash_fwd_params : public Qkv_params {
 
     void * __restrict__ alibi_slopes_ptr;
     index_t alibi_slopes_batch_stride;
+
+    // Cuda Device Properties
+    const cudaDeviceProp* dprop;
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
@@ -179,6 +173,8 @@ struct Flash_bwd_params : public Flash_fwd_params {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
+void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream, bool force_split_kernel=false);
+
 template<typename T, int Headdim> void run_mha_fwd_(Flash_fwd_params &params, cudaStream_t stream);
 template<typename T, int Headdim> void run_mha_fwd_splitkv_dispatch(Flash_fwd_params &params, cudaStream_t stream);
 
diff --git a/csrc/flash_attn/src/flash_fwd_kernel.h b/csrc/flash_attn/src/flash_fwd_kernel.h
index ee9b80a..c745ade 100644
--- a/csrc/flash_attn/src/flash_fwd_kernel.h
+++ b/csrc/flash_attn/src/flash_fwd_kernel.h
@@ -110,11 +110,11 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
     if ((Is_causal || Is_local || !Is_even_MN) && n_block_max <= n_block_min) {
         // Save seed and offset for backward. If we don't have this here, the 0-th thread block might
         // exit early and no one saves the rng state.
-        if (Is_dropout && blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0 && tidx == 0) {
-            auto seeds = at::cuda::philox::unpack(params.philox_args);
-            params.rng_state[0] = std::get<0>(seeds);
-            params.rng_state[1] = std::get<1>(seeds);
-        }
+        // if (Is_dropout && blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0 && tidx == 0) {
+        //     auto seeds = at::cuda::philox::unpack(params.philox_args);
+        //     params.rng_state[0] = std::get<0>(seeds);
+        //     params.rng_state[1] = std::get<1>(seeds);
+        // }
         const index_t row_offset_o = binfo.q_offset(params.o_batch_stride, params.o_row_stride, bidb)
             + m_block * kBlockM * params.o_row_stride + bidh * params.o_head_stride;
         const index_t row_offset_lse = (bidb * params.h + bidh) * params.seqlen_q + m_block * kBlockM;
@@ -332,6 +332,7 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
         cute::copy(smem_tiled_copy_Q, tSsQ, tSrQ_copy_view);
     }
 
+#if 0
     auto seeds = at::cuda::philox::unpack(params.philox_args);
     unsigned long long seed = std::get<0>(seeds);
     unsigned long long offset = std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32;
@@ -341,6 +342,7 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
         params.rng_state[0] = seed;
         params.rng_state[1] = std::get<1>(seeds);
     }
+#endif
 
     clear(acc_o);
 
@@ -447,6 +449,7 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
 
         //PPU: move data convert after dropout, dropout need use ppu C-layout as random result.
 #ifdef USE_PPU
+#if 0
         int block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
         int block_col_idx = n_block * (kBlockN / 32);
         if (Return_softmax) {
@@ -469,12 +472,14 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
             flash::apply_dropout(acc_s_drop, params.p_dropout_in_uint8_t, seed, offset,
                                  block_row_idx, block_col_idx, kNWarps);
         }
+#endif
 
         //PPU: convert output C layout to input A and data type convert.
         Tensor rP = flash::convert_acc<Element>(scores);
         Tensor tOrP = make_tensor(rP.data(), make_layout(get<0>(tSrQ.layout()), get<1>(acc_s.layout()), get<2>(acc_s.layout())));
         // if (cute::thread0()) { print(tOrP); }
 #else
+#error
         // Convert scores from fp32 to fp16/bf16
         Tensor rP = flash::convert_type<Element>(scores);
         // Reshape rP from (nrow=(2, MMA_M), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_M, MMA_N / 2)
@@ -566,6 +571,7 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
 
         //PPU: move data convert after dropout, dropout need use ppu C-layout as random result.
 #ifdef USE_PPU
+#if 0
         int block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
         int block_col_idx = n_block * (kBlockN / 32);
         if (Return_softmax) {
@@ -588,6 +594,7 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
             flash::apply_dropout(acc_s_drop, params.p_dropout_in_uint8_t, seed, offset,
                                  block_row_idx, block_col_idx, kNWarps);
         }
+#endif
 
         //PPU: convert output C layout to input A and data type convert.
         Tensor rP = flash::convert_acc<Element>(scores);
diff --git a/csrc/flash_attn/src/flash_fwd_launch_template.h b/csrc/flash_attn/src/flash_fwd_launch_template.h
index 4437cdb..584890c 100644
--- a/csrc/flash_attn/src/flash_fwd_launch_template.h
+++ b/csrc/flash_attn/src/flash_fwd_launch_template.h
@@ -4,12 +4,23 @@
 
 #pragma once
 
-#include <ATen/cuda/CUDAContext.h>
+#include "cuda.h"
 
 #include "static_switch.h"
 #include "flash.h"
 #include "flash_fwd_kernel.h"
 
+#define CUDA_CHECK(status)                                                                         \
+    {                                                                                              \
+        cudaError_t error = status;                                                                \
+        if (error != cudaSuccess) {                                                                \
+            std::cerr << "Got bad cuda status: " << cudaGetErrorString(error)                      \
+                      << " at line: " << __LINE__ << std::endl;                                    \
+            exit(EXIT_FAILURE);                                                                    \
+        }                                                                                          \
+    }
+#define CUDA_KERNEL_LAUNCH_CHECK() CUDA_CHECK(cudaGetLastError())
+
 template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_local, bool Has_alibi, bool Is_even_MN, bool Is_even_K, bool Return_softmax>
 __global__ void flash_fwd_kernel(Flash_fwd_params params) {
     static_assert(!(Is_causal && Is_local));  // If Is_local is true, Is_causal should be false
@@ -56,7 +67,7 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {
                         // printf("IsEvenMNConst = %d, IsEvenKConst = %d, Is_local = %d, Is_causal = %d, ReturnSoftmaxConst = %d, Is_dropout = %d\n", int(IsEvenMNConst), int(IsEvenKConst), int(Is_local), int(Is_causal), int(ReturnSoftmaxConst), int(Is_dropout));
                         // auto kernel = &flash_fwd_kernel<Kernel_traits, false, Is_causal, false, true, true, false>;
                         if (smem_size >= 48 * 1024) {
-                            C10_CUDA_CHECK(cudaFuncSetAttribute(
+                            CUDA_CHECK(cudaFuncSetAttribute(
                                 kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
                         }
                         // int ctas_per_sm;
@@ -64,7 +75,7 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {
                         //     &ctas_per_sm, kernel, Kernel_traits::kNThreads, smem_size);
                         // printf("smem_size = %d, CTAs per SM = %d\n", int(smem_size), ctas_per_sm);
                         kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
-                        C10_CUDA_KERNEL_LAUNCH_CHECK();
+                        CUDA_KERNEL_LAUNCH_CHECK();
                     });
                 });
             });
@@ -95,11 +106,11 @@ void run_flash_splitkv_fwd(Flash_fwd_params &params, cudaStream_t stream) {
                                 // auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, true, Split, Append_KV>;
                                 // auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, IsEvenKConst>;
                                 if (smem_size >= 48 * 1024) {
-                                    C10_CUDA_CHECK(cudaFuncSetAttribute(
+                                    CUDA_CHECK(cudaFuncSetAttribute(
                                         kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
                                 }
                                 kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
-                                C10_CUDA_KERNEL_LAUNCH_CHECK();
+                                CUDA_KERNEL_LAUNCH_CHECK();
                             });
                         });
                     });
@@ -129,7 +140,7 @@ void run_flash_splitkv_fwd(Flash_fwd_params &params, cudaStream_t stream) {
             } else if (params.num_splits <= 128) {
                 flash_fwd_splitkv_combine_kernel<Kernel_traits, kBlockM, 7, IsEvenKConst><<<grid_combine, Kernel_traits::kNThreads, 0, stream>>>(params);
             }
-            C10_CUDA_KERNEL_LAUNCH_CHECK();
+            CUDA_KERNEL_LAUNCH_CHECK();
         });
     }
 }
@@ -179,7 +190,7 @@ void run_mha_fwd_hdim64(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim96(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 96;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
+    auto dprops = params.dprop;
     bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
@@ -205,7 +216,7 @@ void run_mha_fwd_hdim96(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim128(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 128;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
+    auto dprops = params.dprop;
     bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
@@ -242,7 +253,7 @@ void run_mha_fwd_hdim128(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim160(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 160;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
+    auto dprops = params.dprop;
     bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
@@ -297,7 +308,7 @@ void run_mha_fwd_hdim224(Flash_fwd_params &params, cudaStream_t stream) {
     cudaError status_ = cudaDeviceGetAttribute(
         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);
     if (status_ != cudaSuccess) {
-      C10_CUDA_CHECK(status_);
+      CUDA_CHECK(status_);
     }
     // printf("max_smem_per_block = %d\n", max_smem_per_block);
     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
@@ -328,7 +339,7 @@ void run_mha_fwd_hdim256(Flash_fwd_params &params, cudaStream_t stream) {
     status_ = cudaDeviceGetAttribute(
         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);
     if (status_ != cudaSuccess) {
-      C10_CUDA_CHECK(status_);
+      CUDA_CHECK(status_);
     }
     // printf("max_smem_per_sm = %d, max_smem_per_block = %d\n", max_smem_per_sm, max_smem_per_block);
     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
